# AUTOMATED IMAGE CAPTIONING SYSTEM

Let's imagine one day while going to market you get to see a blind man finding it difficult to cross the road crowded with people and vechiles. How you will help that person in this situation. One way to help is to first ask his permission, make him hold your arm and then help him to cross the road. But is it permanent solution? What about the situation if heis not enough comfortable to accept your help and hold your arm? Or what if there is no one around him to offer such help?

One of the possible permanent solution is of "automated guiding system", which is under research and development. This system in near future will take input of a captured video from eye-wear or head-gear used by blind people and will give a decription about ongoing activities in that video in form of text format captions which will then be converted to audio format. 

But before contributing any idea to this ongoing research we have to first understand the concept of "automated image captioning system". This is important to understand because generating captions for images quickly, accurately and in cost efficient manner will lead towards an efficient "automated guiding system" (which will work on input of captured video,i.e series of time-framed images).

## Let us now see the capability of "automated image captioning system" coded by me:-

![bandicam 2020-10-23 10-59-22-783](https://user-images.githubusercontent.com/71775151/96964503-30691400-1528-11eb-9b5a-821e9984aa33.jpg)

![bandicam 2020-10-23 11-01-07-074](https://user-images.githubusercontent.com/71775151/96965333-97d39380-1529-11eb-8f0e-1ac51c337c4d.jpg)

![bandicam 2020-10-23 11-19-08-501](https://user-images.githubusercontent.com/71775151/96965388-b0dc4480-1529-11eb-89d9-5c3dbb037e0d.jpg)

![bandicam 2020-10-23 11-31-19-095](https://user-images.githubusercontent.com/71775151/96965447-cb162280-1529-11eb-8d54-4b1932950972.jpg)

## Are you just shocked and amazed seeing these generated captions?? Want to give it a try on any of your image?? 

Well in a minute I will let you to experience this amazing moment all by yourself but before that I want to re-state the meaning of "any image" as of present situation. See as this model/system is trained on "Flick8k" dataset with just 8,000 images thus you can't expect it to be so generalised that it will understand each object in picture correctly everytime. 

Still have question that 8,000 images are sooo much, then also why this model isn't generalised? To make you understand this limitation I just want you to make a count that till now how many objects have you seen and know their identification? Maybe quite large that you can't even count in one go, is it so? So how can you expect this model to be so generalised with just 8,000 images, isn't this unfair?   

And before moving ahead towards code, I would let you know that I could have trained this model on larger dataset like "MS-COCO" having 300,000+ unique images. But being an undergraduate student I have limited compuational resources. So will try to improve this model further when will have my first salary cheque in my hand.

Now, I think I have told you enough about limitations of my model as of present state, but to be honest this model isn't that bad.

## Time to give you first hand experience of generating caption for image of you choice, but consider the note I have given above.
Ok so to make you try it on your own I would like you to copy the trained weights and some essential pickeled weights to your Google Drive. No need to worry just follow the following steps:-

**STEP 1)** Click on this link:- https://drive.google.com/drive/folders/1Nw4H5rttfy1O8Qf-GIGkqUOBeyoHKSOg?usp=sharing. 
STEP 2) Now open your Google Drive and click on "Shared with me" present at your left hand side.
